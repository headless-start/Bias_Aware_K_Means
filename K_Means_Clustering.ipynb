{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "f9e5a495-5e0d-4256-971f-9f75efd8d369",
      "cell_type": "code",
      "source": "from multiprocessing import Pool\nimport time\nimport numpy as np\nimport psutil\nfrom sklearn.metrics import silhouette_score\nfrom greedy_clustering import greedy_clustering, euclidean\n\n\nREP_FILE = 'representatives.csv'\nASGNMT_FILE = 'assignments.csv'\nK_MEANS_CENTROID_FILE = 'kmeans_centroids.csv'\nK_MEANS_LABELS_FILE = 'kmeans_labels.csv'\nFINAL_ASSGMNT_GREEDY_FILE = 'final_a1.csv'\nFINAL_ASSGMNT_SUBSAMPLING_FILE = 'final_a2.csv'\n\n\ndef subsampling(file_path, ratio, seed):\n    \"\"\"\n    Baseline A2 approach: randomly subsample 'ratio' fraction of the data\n    to form the reduced set R. Saves R to representatives.csv\n    \"\"\"\n    print(\"Reading full data for subsampling...\")\n    data = np.loadtxt(file_path, delimiter=',')   # shape (N, d)\n    rng = np.random.default_rng(seed=seed)\n\n    # Number of points to sample\n    sample_size = int(ratio * len(data))\n\n    # Pick 'sample_size' distinct random indices\n    indices = rng.choice(len(data), size=sample_size, replace=False)\n    representatives = data[indices]               # shape (sample_size, d)\n\n    # Save to the same file name used by the greedy approach, for uniformity\n    np.savetxt(REP_FILE, representatives, delimiter=\",\")\n    print(f\"Subsampled R saved to {REP_FILE} with shape {representatives.shape}\")\n    return None\n\n### TODO: Add/Change functions below\ndef _assign_chunk(chunk, centroids):\n    \"\"\"\n    Helper for parallel assignment step in K-Means.\n    Returns a list of cluster labels (nearest centroid) for each point in 'chunk'.\n    \"\"\"\n    labels_chunk = []\n    for x in chunk:\n        dists = [euclidean(x, c) for c in centroids]\n        labels_chunk.append(np.argmin(dists))\n    return labels_chunk\n\n# Helper function to monitor memory usage\ndef monitor_memory():\n    process = psutil.Process()\n    return process.memory_info().rss / (1024 * 1024)  # Memory in MB\n\n# Evaluate clustering quality\ndef evaluate_clustering(assignments_file, centroid_file, data_file):\n    assignments = np.loadtxt(assignments_file, delimiter=\",\")\n    centroids = np.loadtxt(centroid_file, delimiter=\",\")\n    data = np.loadtxt(data_file, delimiter=\",\")\n\n    # Silhouette Score\n    labels = assignments[:, -1]  # Last column contains cluster labels\n    silhouette = silhouette_score(data, labels)\n    \n    # Compute intra-cluster and inter-cluster distances\n    intra_distances = []\n    for i, centroid in enumerate(centroids):\n        points = data[labels == i]\n        if len(points) > 0:\n            intra_distances.append(np.linalg.norm(points - centroid, axis=1).mean())\n    intra_distance = np.mean(intra_distances)\n\n    inter_distances = [\n        np.linalg.norm(centroids[i] - centroids[j])\n        for i in range(len(centroids)) for j in range(i + 1, len(centroids))\n    ]\n    inter_distance = np.mean(inter_distances)\n\n    return {\n        \"silhouette_score\": silhouette,\n        \"intra_cluster_distance\": intra_distance,\n        \"inter_cluster_distance\": inter_distance\n    }\n\n# Function to run comparisons for A1 and A2\ndef compare_methods(file_path):\n    metrics = {}\n    clustering_results = {}\n\n    # Run A1 (Greedy Clustering)\n    print(\"Comparing A1 (Greedy Clustering)...\")\n    start_time = time.time()\n    start_mem = monitor_memory()\n    tau = 100\n    greedy_clustering(file_path, tau, seed=1, nr_workers=4)\n    k_means(REP_FILE, K=3, max_iters=100, seed=1)\n    end_time = time.time()\n    end_mem = monitor_memory()\n\n    metrics['A1'] = {\n        \"time\": end_time - start_time,\n        \"memory\": end_mem - start_mem\n    }\n\n    clustering_results['A1'] = evaluate_clustering(\n        assignments_file=FINAL_ASSGMNT_GREEDY_FILE,\n        centroid_file=K_MEANS_CENTROID_FILE,\n        data_file=file_path\n    )\n\n    # Run A2 (Subsampling)\n    print(\"Comparing A2 (Subsampling)...\")\n    start_time = time.time()\n    start_mem = monitor_memory()\n    ratio = 0.10  # Example: 10% of data\n    subsampling(file_path, ratio=ratio, seed=1)\n    k_means(REP_FILE, K=3, max_iters=100, seed=1)\n    end_time = time.time()\n    end_mem = monitor_memory()\n\n    metrics['A2'] = {\n        \"time\": end_time - start_time,\n        \"memory\": end_mem - start_mem\n    }\n\n    clustering_results['A2'] = evaluate_clustering(\n        assignments_file=FINAL_ASSGMNT_SUBSAMPLING_FILE,\n        centroid_file=K_MEANS_CENTROID_FILE,\n        data_file=file_path\n    )\n\n    # Print results\n    print(\"\\nResource Usage Comparison:\")\n    for method, usage in metrics.items():\n        print(f\"Method: {method}\")\n        print(f\"  Time: {usage['time']} seconds\")\n        print(f\"  Memory: {usage['memory']} MB\")\n\n    print(\"\\nClustering Quality Metrics:\")\n    for method, results in clustering_results.items():\n        print(f\"Method: {method}\")\n        print(f\"  Silhouette Score: {results['silhouette_score']}\")\n        print(f\"  Intra-cluster Distance: {results['intra_cluster_distance']}\")\n        print(f\"  Inter-cluster Distance: {results['inter_cluster_distance']}\")\n\n### TODO: Add/Change functions above\n\ndef k_means(R_file, K, max_iters, seed):\n    \"\"\"\n    Perform K-Means on the reduced dataset R (read from R_file).\n    Saves final centroids and labels:\n        - kmeans_centroids.csv  (shape: (K, d))\n        - kmeans_labels.csv     (shape: (|R|,))\n    The order of labels corresponds to the order of points in R.\n    \"\"\"\n    print(\"Reading representatives for K-Means...\")\n    nr_workers = 4\n    R = np.loadtxt(R_file, delimiter=\",\")  # shape: (|R|, d)\n    rng = np.random.default_rng(seed=seed)\n    N, d = R.shape\n\n    # Initialize K centroids by picking K random points from R\n    idx = rng.choice(N, size=K, replace=False)\n    centroids = R[idx].copy()  # shape: (K, d)\n\n    # Initialize cluster labels array\n    labels = np.zeros(N, dtype=int)\n\n    # K-Means loop\n    for iteration in range(max_iters):\n        # E-Step: assign each point in R to nearest centroid\n        if nr_workers > 1:\n            # Parallel approach\n            chunks = np.array_split(R, nr_workers, axis=0)\n            with Pool(processes=nr_workers) as pool:\n                partial_labels = pool.starmap(_assign_chunk, [(chunk, centroids) \n                                                              for chunk in chunks])\n            # Flatten the partial labels back\n            labels = np.concatenate([np.array(lbls) for lbls in partial_labels])\n        else:\n            # Single-thread approach\n            for i in range(N):\n                dists = [euclidean(R[i], c) for c in centroids]\n                labels[i] = np.argmin(dists)\n\n        # M-Step: recompute each centroid as mean of assigned points\n        new_centroids = np.zeros_like(centroids)\n        for k in range(K):\n            points_k = R[labels == k]\n            if len(points_k) > 0:\n                new_centroids[k] = np.mean(points_k, axis=0)\n            else:\n                # If no points assigned to cluster k, re-initialize randomly\n                new_centroids[k] = R[rng.integers(N)]\n\n        # Check for convergence\n        if np.allclose(new_centroids, centroids):\n            print(f\"Converged at iteration {iteration}\")\n            centroids = new_centroids\n            break\n        centroids = new_centroids\n\n    # Save results\n    np.savetxt(K_MEANS_CENTROID_FILE, centroids, delimiter=\",\")\n    np.savetxt(K_MEANS_LABELS_FILE, labels, delimiter=\",\")\n    print(f\"K-Means finished. Saved centroids to {K_MEANS_CENTROID_FILE} and labels to {K_MEANS_LABELS_FILE}.\")\n    return None\n\n\ndef remap_labels(kmeans_labels_file, kmeans_centroids_file, assignments_file, dataset_path, subsample):\n    if not subsample:\n        ### NOTE: assignments shape is (num_data_points, 7)\n        assignments = np.loadtxt(assignments_file, delimiter=\",\")\n    else:\n        ### NOTE: assignments shape is (num_data_points, 6)\n        ### NOTE: since the subsampling doesn't save the assignments, it is only the datapoints.\n        assignments = np.loadtxt(dataset_path, delimiter=\",\")\n    \n    kmeans_centroids = np.loadtxt(kmeans_centroids_file, delimiter=\",\")\n    \n    ### NOTE: kmeans_labels has length num_representatives\n    kmeans_labels = np.loadtxt(kmeans_labels_file, delimiter=\",\")\n\n    ### NOTE: X_labels has length num_data_points\n    X_labels = np.zeros(len(assignments), dtype=int)\n    ### NOTE: final_assignments has shape (num_data_points,7) , 6 dimensions for data points and 1 dimension for cluster label of kmeans\n    final_assignments = np.zeros((len(assignments), 7))\n\n    ### TODO: Insert code below\n    if not subsample:\n        N = len(assignments)\n        for i in range(N):\n            # Which representative was this point assigned to?\n            rep_index = int(assignments[i, 6])\n            # That rep_index maps to kmeans_labels[rep_index]\n            cluster_label = kmeans_labels[rep_index]\n            # Fill the final assignments\n            final_assignments[i, :6] = assignments[i, :6]  # original data\n            final_assignments[i, 6] = cluster_label\n    else:\n        X = np.loadtxt(dataset_path, delimiter=\",\")  # shape (N, d)\n        R = np.loadtxt(REP_FILE, delimiter=\",\")      # shape (|R|, d)\n        N = len(X)\n        for i, x in enumerate(X):\n            # Find nearest subsampled representative\n            dists = [euclidean(x, r) for r in R]\n            nearest_rep = np.argmin(dists)\n            cluster_label = kmeans_labels[nearest_rep]\n            final_assignments[i, :6] = x\n            final_assignments[i, 6] = cluster_label\n    ### TODO: Insert code above\n\n    if subsample:\n        np.savetxt(FINAL_ASSGMNT_SUBSAMPLING_FILE, final_assignments, delimiter=\",\")\n    else:\n        np.savetxt(FINAL_ASSGMNT_GREEDY_FILE, final_assignments, delimiter=\",\")\n\n    return None\n\nif __name__ == \"__main__\":\n    file_path = \"small_dataset.csv\"\n    subsample = True\n\n    if subsample:\n        # A2: Subsampling\n        ratio = 0.10   # ex 10% of data\n        subsampling(file_path, ratio=ratio, seed=1)\n    else:\n        # A1: Greedy clustering\n        tau = 100\n        greedy_clustering(file_path, tau, seed=1, nr_workers=4)\n\n    # Run K-Means on the reduced data in representatives.csv\n    K = 3\n    max_iters = 100\n    k_means(REP_FILE, K=K, max_iters=max_iters, seed=1)\n\n    # Postprocessing: map cluster labels back to the full dataset\n    # For greedy approach => it uses assignments.csv\n    # For subsampling    => it reads directly from file_path\n    remap_labels(\n        kmeans_labels_file=K_MEANS_LABELS_FILE,\n        kmeans_centroids_file=K_MEANS_CENTROID_FILE,\n        assignments_file=ASGNMT_FILE,\n        dataset_path=file_path,\n        subsample=subsample\n    )\n\n    # Run comparison after main functionalities\n    print(\"\\nStarting A1 vs A2 comparison...\")\n    compare_methods(file_path)\n    print(\"Done.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}